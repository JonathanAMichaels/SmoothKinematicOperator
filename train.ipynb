{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from model import BiGRU, CustomLoss, preprocess_data, CSVDataset, plot_heatmaps, preprocess_for_test\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Read CSV file and load into a pandas DataFrame\n",
    "save_folder = '/Users/jonathanamichaels/Library/CloudStorage/Dropbox'\n",
    "csv_file = save_folder + '/data3D.csv'\n",
    "data = pd.read_csv(csv_file)\n",
    "norm_constant = 1000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Batch: 1/13, Loss: 0.03115137293934822\n",
      "Epoch: 1/50, Batch: 2/13, Loss: 0.023346034809947014\n",
      "Epoch: 1/50, Batch: 3/13, Loss: 0.01905081979930401\n",
      "Epoch: 1/50, Batch: 4/13, Loss: 0.010391129180788994\n",
      "Epoch: 1/50, Batch: 5/13, Loss: 0.007071503438055515\n",
      "Epoch: 1/50, Batch: 6/13, Loss: 0.006996306125074625\n",
      "Epoch: 1/50, Batch: 7/13, Loss: 0.006300672423094511\n",
      "Epoch: 1/50, Batch: 8/13, Loss: 0.006515518296509981\n",
      "Epoch: 1/50, Batch: 9/13, Loss: 0.00537166278809309\n",
      "Epoch: 1/50, Batch: 10/13, Loss: 0.0045268055982887745\n",
      "Epoch: 1/50, Batch: 11/13, Loss: 0.00436385115608573\n",
      "Epoch: 1/50, Batch: 12/13, Loss: 0.003996977582573891\n",
      "Epoch: 1/50, Batch: 13/13, Loss: 0.005107490345835686\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 54\u001B[0m\n\u001B[1;32m     51\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, targets, confidence)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# Backpropagation\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# Update the weights\u001B[39;00m\n\u001B[1;32m     57\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m~/miniconda/envs/MotorNet/lib/python3.8/site-packages/torch/_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    486\u001B[0m     )\n\u001B[0;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda/envs/MotorNet/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "processed_data = preprocess_data(data, norm_constant=norm_constant)\n",
    "\n",
    "# Set the model parameters\n",
    "input_size = processed_data.shape[2]\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "output_size = processed_data.shape[2]\n",
    "dropout = 0.1\n",
    "\n",
    "# Initialize the model and loss function\n",
    "model = BiGRU(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CSVDataset(processed_data, norm_constant=norm_constant)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (inputs, targets, confidence) in enumerate(dataloader):\n",
    "         # Move tensors to the appropriate device (if using GPU)\n",
    "        inputs, targets, confidence = inputs.to(device), targets.to(device), confidence.to(device)\n",
    "\n",
    "        # Reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Model output and extra information\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, targets, confidence)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss for the current batch\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Batch: {batch_idx+1}/{len(dataloader)}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data = preprocess_for_test(data, norm_constant=norm_constant)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(test_data)\n",
    "plot_heatmaps(test_data[0,:,:], output[0,:,:].detach().numpy())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_aug = pd.DataFrame.copy(data)\n",
    "# normalize\n",
    "data_aug.loc[1:,:] = output.detach().numpy() * norm_constant\n",
    "# fix magnitude of confidence\n",
    "data_aug.iloc[1:,3::4] = data_aug.iloc[1:, 3::4]  / norm_constant\n",
    "\n",
    "# if you don't want to interpolate time points with no centerdetect\n",
    "if False:\n",
    "    nan_indices = data.isna().stack()[lambda x: x].index.tolist()\n",
    "    for row, col in nan_indices:\n",
    "        data_aug.at[row, col] = pd.NA\n",
    "\n",
    "data_aug.to_csv(save_folder + '/data3D_Smooth.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
