{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-12T20:25:31.260679766Z",
     "start_time": "2023-05-12T20:25:31.191821075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from model import BiGRU, CustomLoss, preprocess_data, CSVDataset, plot_heatmaps, preprocess_for_test, get_normalization_values, normalize_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_988189/3870571526.py:4: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(csv_file)\n"
     ]
    }
   ],
   "source": [
    "# Point the save_folder to your predictions\n",
    "save_folder = '/home/jonathan/Desktop/JARVIS-HybridNet/projects/May11/predictions/predictions3D/Predictions_3D_20230512-160305'\n",
    "csv_file = save_folder + '/data3D.csv'\n",
    "data = pd.read_csv(csv_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-12T20:25:31.523258848Z",
     "start_time": "2023-05-12T20:25:31.253942573Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500, Loss: 0.5251713991165161\n",
      "Epoch: 2/500, Loss: 0.1712782233953476\n",
      "Epoch: 3/500, Loss: 0.04741213470697403\n",
      "Epoch: 4/500, Loss: 0.09679674357175827\n",
      "Epoch: 5/500, Loss: 0.05157368257641792\n",
      "Epoch: 6/500, Loss: 0.03590139001607895\n",
      "Epoch: 7/500, Loss: 0.03842749446630478\n",
      "Epoch: 8/500, Loss: 0.037597160786390305\n",
      "Epoch: 9/500, Loss: 0.037142761051654816\n",
      "Epoch: 10/500, Loss: 0.03266436234116554\n",
      "Epoch: 11/500, Loss: 0.024364100769162178\n",
      "Epoch: 12/500, Loss: 0.016751114279031754\n",
      "Epoch: 13/500, Loss: 0.021320553496479988\n",
      "Epoch: 14/500, Loss: 0.022710613906383514\n",
      "Epoch: 15/500, Loss: 0.019821632653474808\n",
      "Epoch: 16/500, Loss: 0.01280167419463396\n",
      "Epoch: 17/500, Loss: 0.01293257437646389\n",
      "Epoch: 18/500, Loss: 0.011456234380602837\n",
      "Epoch: 19/500, Loss: 0.014420527964830399\n",
      "Epoch: 20/500, Loss: 0.014393510296940804\n",
      "Epoch: 21/500, Loss: 0.014930264092981815\n",
      "Epoch: 22/500, Loss: 0.011181199923157692\n",
      "Epoch: 23/500, Loss: 0.010414975695312023\n",
      "Epoch: 24/500, Loss: 0.0076769208535552025\n",
      "Epoch: 25/500, Loss: 0.009525207802653313\n",
      "Epoch: 26/500, Loss: 0.009761146269738674\n",
      "Epoch: 27/500, Loss: 0.007684045005589724\n",
      "Epoch: 28/500, Loss: 0.010082251392304897\n",
      "Epoch: 29/500, Loss: 0.006941466126590967\n",
      "Epoch: 30/500, Loss: 0.007622983306646347\n",
      "Epoch: 31/500, Loss: 0.007508773356676102\n",
      "Epoch: 32/500, Loss: 0.00615309365093708\n",
      "Epoch: 33/500, Loss: 0.006288456730544567\n",
      "Epoch: 34/500, Loss: 0.007900374010205269\n",
      "Epoch: 35/500, Loss: 0.006657259073108435\n",
      "Epoch: 36/500, Loss: 0.005886514205485582\n",
      "Epoch: 37/500, Loss: 0.004883327521383762\n",
      "Epoch: 38/500, Loss: 0.00588744692504406\n",
      "Epoch: 39/500, Loss: 0.004291431047022343\n",
      "Epoch: 40/500, Loss: 0.005733071360737085\n",
      "Epoch: 41/500, Loss: 0.005849718581885099\n",
      "Epoch: 42/500, Loss: 0.004048533271998167\n",
      "Epoch: 43/500, Loss: 0.003976697567850351\n",
      "Epoch: 44/500, Loss: 0.003919077105820179\n",
      "Epoch: 45/500, Loss: 0.0046032327227294445\n",
      "Epoch: 46/500, Loss: 0.003914407920092344\n",
      "Epoch: 47/500, Loss: 0.0038688653148710728\n",
      "Epoch: 48/500, Loss: 0.003477951278910041\n",
      "Epoch: 49/500, Loss: 0.004307004623115063\n",
      "Epoch: 50/500, Loss: 0.0034812157973647118\n",
      "Epoch: 51/500, Loss: 0.004241600167006254\n",
      "Epoch: 52/500, Loss: 0.002808796474710107\n",
      "Epoch: 53/500, Loss: 0.00319496332667768\n",
      "Epoch: 54/500, Loss: 0.0031055582221597433\n",
      "Epoch: 55/500, Loss: 0.003816812764853239\n",
      "Epoch: 56/500, Loss: 0.0036644190549850464\n",
      "Epoch: 57/500, Loss: 0.0035704541951417923\n",
      "Epoch: 58/500, Loss: 0.003363701980561018\n",
      "Epoch: 59/500, Loss: 0.0025860301684588194\n",
      "Epoch: 60/500, Loss: 0.0034174390602856874\n",
      "Epoch: 61/500, Loss: 0.0030863420106470585\n",
      "Epoch: 62/500, Loss: 0.003445869544520974\n",
      "Epoch: 63/500, Loss: 0.002835705876350403\n",
      "Epoch: 64/500, Loss: 0.0028603505343198776\n",
      "Epoch: 65/500, Loss: 0.0031716690864413977\n",
      "Epoch: 66/500, Loss: 0.0021840985864400864\n",
      "Epoch: 67/500, Loss: 0.0027824710123240948\n",
      "Epoch: 68/500, Loss: 0.002537772059440613\n",
      "Epoch: 69/500, Loss: 0.002459312556311488\n",
      "Epoch: 70/500, Loss: 0.0023808185942471027\n",
      "Epoch: 71/500, Loss: 0.0027209429536014795\n",
      "Epoch: 72/500, Loss: 0.0022330936044454575\n",
      "Epoch: 73/500, Loss: 0.0025980740319937468\n",
      "Epoch: 74/500, Loss: 0.0023897874634712934\n",
      "Epoch: 75/500, Loss: 0.002432584296911955\n",
      "Epoch: 76/500, Loss: 0.002492327941581607\n",
      "Epoch: 77/500, Loss: 0.002044453751295805\n",
      "Epoch: 78/500, Loss: 0.0023867154959589243\n",
      "Epoch: 79/500, Loss: 0.002297863829880953\n",
      "Epoch: 80/500, Loss: 0.0020760404877364635\n",
      "Epoch: 81/500, Loss: 0.0023513068445026875\n",
      "Epoch: 82/500, Loss: 0.0024408793542534113\n",
      "Epoch: 83/500, Loss: 0.002385055413469672\n",
      "Epoch: 84/500, Loss: 0.0021589056123048067\n",
      "Epoch: 85/500, Loss: 0.002088336506858468\n",
      "Epoch: 86/500, Loss: 0.0022832555696368217\n",
      "Epoch: 87/500, Loss: 0.002252118429169059\n",
      "Epoch: 88/500, Loss: 0.0024566405918449163\n",
      "Epoch: 89/500, Loss: 0.0026505214627832174\n",
      "Epoch: 90/500, Loss: 0.001889633247628808\n",
      "Epoch: 91/500, Loss: 0.002088121371343732\n",
      "Epoch: 92/500, Loss: 0.0019111448200419545\n",
      "Epoch: 93/500, Loss: 0.0017293512355536222\n",
      "Epoch: 94/500, Loss: 0.0017565969610586762\n",
      "Epoch: 95/500, Loss: 0.0019732704386115074\n",
      "Epoch: 96/500, Loss: 0.0019881525076925755\n",
      "Epoch: 97/500, Loss: 0.0023361428175121546\n",
      "Epoch: 98/500, Loss: 0.0021568285301327705\n",
      "Epoch: 99/500, Loss: 0.0018761131213977933\n",
      "Epoch: 100/500, Loss: 0.0018152990378439426\n",
      "Epoch: 101/500, Loss: 0.0021650968119502068\n",
      "Epoch: 102/500, Loss: 0.001877050963230431\n",
      "Epoch: 103/500, Loss: 0.0019846190698444843\n",
      "Epoch: 104/500, Loss: 0.002144459867849946\n",
      "Epoch: 105/500, Loss: 0.0017932066693902016\n",
      "Epoch: 106/500, Loss: 0.0019418641459196806\n",
      "Epoch: 107/500, Loss: 0.0021184533834457397\n",
      "Epoch: 108/500, Loss: 0.0017534102080389857\n",
      "Epoch: 109/500, Loss: 0.0016364033799618483\n",
      "Epoch: 110/500, Loss: 0.0015805310104042292\n",
      "Epoch: 111/500, Loss: 0.0017402581870555878\n",
      "Epoch: 112/500, Loss: 0.0018662166548892856\n",
      "Epoch: 113/500, Loss: 0.0017538649262860417\n",
      "Epoch: 114/500, Loss: 0.0017151643987745047\n",
      "Epoch: 115/500, Loss: 0.0018748471047729254\n",
      "Epoch: 116/500, Loss: 0.0017805586103349924\n",
      "Epoch: 117/500, Loss: 0.0015156258596107364\n",
      "Epoch: 118/500, Loss: 0.001653375686146319\n",
      "Epoch: 119/500, Loss: 0.0017586861504241824\n",
      "Epoch: 120/500, Loss: 0.0015300917439162731\n",
      "Epoch: 121/500, Loss: 0.0018593146232888103\n",
      "Epoch: 122/500, Loss: 0.0017355326563119888\n",
      "Epoch: 123/500, Loss: 0.0013245869195088744\n",
      "Epoch: 124/500, Loss: 0.0019243430579081178\n",
      "Epoch: 125/500, Loss: 0.0017714400310069323\n",
      "Epoch: 126/500, Loss: 0.0015410480555146933\n",
      "Epoch: 127/500, Loss: 0.0014974756632000208\n",
      "Epoch: 128/500, Loss: 0.0017016830388456583\n",
      "Epoch: 129/500, Loss: 0.0014188175555318594\n",
      "Epoch: 130/500, Loss: 0.0016995426267385483\n",
      "Epoch: 131/500, Loss: 0.0015097238356247544\n",
      "Epoch: 132/500, Loss: 0.0017813944723457098\n",
      "Epoch: 133/500, Loss: 0.0016094574239104986\n",
      "Epoch: 134/500, Loss: 0.0014864716213196516\n",
      "Epoch: 135/500, Loss: 0.0012562978081405163\n",
      "Epoch: 136/500, Loss: 0.0016381527530029416\n",
      "Epoch: 137/500, Loss: 0.001530683133751154\n",
      "Epoch: 138/500, Loss: 0.0015984028577804565\n",
      "Epoch: 139/500, Loss: 0.001461694366298616\n",
      "Epoch: 140/500, Loss: 0.001517601776868105\n",
      "Epoch: 141/500, Loss: 0.0015420480631291866\n",
      "Epoch: 142/500, Loss: 0.0013608301524072886\n",
      "Epoch: 143/500, Loss: 0.0013836091384291649\n",
      "Epoch: 144/500, Loss: 0.0013853406999260187\n",
      "Epoch: 145/500, Loss: 0.0015024986350908875\n",
      "Epoch: 146/500, Loss: 0.0015255610924214125\n",
      "Epoch: 147/500, Loss: 0.00152315862942487\n",
      "Epoch: 148/500, Loss: 0.0014265733771026134\n",
      "Epoch: 149/500, Loss: 0.0010428589303046465\n",
      "Epoch: 150/500, Loss: 0.0015694022877141833\n",
      "Epoch: 151/500, Loss: 0.001540188561193645\n",
      "Epoch: 152/500, Loss: 0.0014259993331506848\n",
      "Epoch: 153/500, Loss: 0.0014614511746913195\n",
      "Epoch: 154/500, Loss: 0.0019347217166796327\n",
      "Epoch: 155/500, Loss: 0.00145795289427042\n",
      "Epoch: 156/500, Loss: 0.00150225346442312\n",
      "Epoch: 157/500, Loss: 0.0014959591208025813\n",
      "Epoch: 158/500, Loss: 0.0012123740743845701\n",
      "Epoch: 159/500, Loss: 0.0014443784020841122\n",
      "Epoch: 160/500, Loss: 0.0013007643865421414\n",
      "Epoch: 161/500, Loss: 0.0015149498358368874\n",
      "Epoch: 162/500, Loss: 0.0015622015343979\n",
      "Epoch: 163/500, Loss: 0.0011764318915084004\n",
      "Epoch: 164/500, Loss: 0.0015833524521440268\n",
      "Epoch: 165/500, Loss: 0.0013691859785467386\n",
      "Epoch: 166/500, Loss: 0.0014240803429856896\n",
      "Epoch: 167/500, Loss: 0.0013024723157286644\n",
      "Epoch: 168/500, Loss: 0.0012591715203598142\n",
      "Epoch: 169/500, Loss: 0.0013895336305722594\n",
      "Epoch: 170/500, Loss: 0.0012261304073035717\n",
      "Epoch: 171/500, Loss: 0.0013567489804700017\n",
      "Epoch: 172/500, Loss: 0.0013885030057281256\n",
      "Epoch: 173/500, Loss: 0.0012344708666205406\n",
      "Epoch: 174/500, Loss: 0.001180400955490768\n",
      "Epoch: 175/500, Loss: 0.0012916020350530744\n",
      "Epoch: 176/500, Loss: 0.001568552223034203\n",
      "Epoch: 177/500, Loss: 0.001403471571393311\n",
      "Epoch: 178/500, Loss: 0.0013749005738645792\n",
      "Epoch: 179/500, Loss: 0.0013733023079112172\n",
      "Epoch: 180/500, Loss: 0.0011073171626776457\n",
      "Epoch: 181/500, Loss: 0.0012644545640796423\n",
      "Epoch: 182/500, Loss: 0.0012378481915220618\n",
      "Epoch: 183/500, Loss: 0.001583663746714592\n",
      "Epoch: 184/500, Loss: 0.0012752999318763614\n",
      "Epoch: 185/500, Loss: 0.0011548104230314493\n",
      "Epoch: 186/500, Loss: 0.0013757022097706795\n",
      "Epoch: 187/500, Loss: 0.0013838791055604815\n",
      "Epoch: 188/500, Loss: 0.0012278355425223708\n",
      "Epoch: 189/500, Loss: 0.001176444347947836\n",
      "Epoch: 190/500, Loss: 0.0016604745760560036\n",
      "Epoch: 191/500, Loss: 0.001153049641288817\n",
      "Epoch: 192/500, Loss: 0.0013548968126997352\n",
      "Epoch: 193/500, Loss: 0.0014590889913961291\n",
      "Epoch: 194/500, Loss: 0.0009934083791449666\n",
      "Epoch: 195/500, Loss: 0.0012034537503495812\n",
      "Epoch: 196/500, Loss: 0.001436919905245304\n",
      "Epoch: 197/500, Loss: 0.0011775981402024627\n",
      "Epoch: 198/500, Loss: 0.0009893361711874604\n",
      "Epoch: 199/500, Loss: 0.0012038592249155045\n",
      "Epoch: 200/500, Loss: 0.001168691087514162\n",
      "Epoch: 201/500, Loss: 0.0012111919932067394\n",
      "Epoch: 202/500, Loss: 0.0012586801312863827\n",
      "Epoch: 203/500, Loss: 0.0011485959403216839\n",
      "Epoch: 204/500, Loss: 0.0011974223889410496\n",
      "Epoch: 205/500, Loss: 0.0012850062921643257\n",
      "Epoch: 206/500, Loss: 0.0010755432303994894\n",
      "Epoch: 207/500, Loss: 0.0014576840912923217\n",
      "Epoch: 208/500, Loss: 0.0010903187794610858\n",
      "Epoch: 209/500, Loss: 0.0013586428249254823\n",
      "Epoch: 210/500, Loss: 0.0010335950646549463\n",
      "Epoch: 211/500, Loss: 0.0013771166559308767\n",
      "Epoch: 212/500, Loss: 0.0010270669590681791\n",
      "Epoch: 213/500, Loss: 0.0011465598363429308\n",
      "Epoch: 214/500, Loss: 0.0011098512914031744\n",
      "Epoch: 215/500, Loss: 0.0011326625244691968\n",
      "Epoch: 216/500, Loss: 0.001394642167724669\n",
      "Epoch: 217/500, Loss: 0.001211049617268145\n",
      "Epoch: 218/500, Loss: 0.0009271950111724436\n",
      "Epoch: 219/500, Loss: 0.0011555644450709224\n",
      "Epoch: 220/500, Loss: 0.0011528240283951163\n",
      "Epoch: 221/500, Loss: 0.0011397985508665442\n",
      "Epoch: 222/500, Loss: 0.0012254827888682485\n",
      "Epoch: 223/500, Loss: 0.0011107615428045392\n",
      "Epoch: 224/500, Loss: 0.001121755689382553\n",
      "Epoch: 225/500, Loss: 0.0010435618460178375\n",
      "Epoch: 226/500, Loss: 0.001000007032416761\n",
      "Epoch: 227/500, Loss: 0.0010907319374382496\n",
      "Epoch: 228/500, Loss: 0.0011930747423321009\n",
      "Epoch: 229/500, Loss: 0.001074947533197701\n",
      "Epoch: 230/500, Loss: 0.0010386606445536017\n",
      "Epoch: 231/500, Loss: 0.0011919797398149967\n",
      "Epoch: 232/500, Loss: 0.0009338819654658437\n",
      "Epoch: 233/500, Loss: 0.0012093334225937724\n",
      "Epoch: 234/500, Loss: 0.001161942956969142\n",
      "Epoch: 235/500, Loss: 0.0011402068194001913\n",
      "Epoch: 236/500, Loss: 0.0009923194302245975\n",
      "Epoch: 237/500, Loss: 0.001054168795235455\n",
      "Epoch: 238/500, Loss: 0.0012715045595541596\n",
      "Epoch: 239/500, Loss: 0.0010671239579096437\n",
      "Epoch: 240/500, Loss: 0.0012221300275996327\n",
      "Epoch: 241/500, Loss: 0.0010848750825971365\n",
      "Epoch: 242/500, Loss: 0.0010735706891864538\n",
      "Epoch: 243/500, Loss: 0.0010414149146527052\n",
      "Epoch: 244/500, Loss: 0.00107338628731668\n",
      "Epoch: 245/500, Loss: 0.0011905445717275143\n",
      "Epoch: 246/500, Loss: 0.0010778176365420222\n",
      "Epoch: 247/500, Loss: 0.0010931241558864713\n",
      "Epoch: 248/500, Loss: 0.0010058983461931348\n",
      "Epoch: 249/500, Loss: 0.001222276478074491\n",
      "Epoch: 250/500, Loss: 0.0009987965459004045\n",
      "Epoch: 251/500, Loss: 0.000888167240191251\n",
      "Epoch: 252/500, Loss: 0.0011344170197844505\n",
      "Epoch: 253/500, Loss: 0.0010496649192646146\n",
      "Epoch: 254/500, Loss: 0.001187934074550867\n",
      "Epoch: 255/500, Loss: 0.00101098685991019\n",
      "Epoch: 256/500, Loss: 0.0008661849424242973\n",
      "Epoch: 257/500, Loss: 0.0010838358430191875\n",
      "Epoch: 258/500, Loss: 0.0011193034006282687\n",
      "Epoch: 259/500, Loss: 0.0010235365480184555\n",
      "Epoch: 260/500, Loss: 0.001100350869819522\n",
      "Epoch: 261/500, Loss: 0.000988017418421805\n",
      "Epoch: 262/500, Loss: 0.00120451464317739\n",
      "Epoch: 263/500, Loss: 0.0011096671223640442\n",
      "Epoch: 264/500, Loss: 0.0009427743498235941\n",
      "Epoch: 265/500, Loss: 0.0010728747583925724\n",
      "Epoch: 266/500, Loss: 0.0009972605621442199\n",
      "Epoch: 267/500, Loss: 0.0009930430678650737\n",
      "Epoch: 268/500, Loss: 0.001057468238286674\n",
      "Epoch: 269/500, Loss: 0.0009118672460317612\n",
      "Epoch: 270/500, Loss: 0.0010559678776189685\n",
      "Epoch: 271/500, Loss: 0.0010371828684583306\n",
      "Epoch: 272/500, Loss: 0.0011993302032351494\n",
      "Epoch: 273/500, Loss: 0.0009636036702431738\n",
      "Epoch: 274/500, Loss: 0.0010701428400352597\n",
      "Epoch: 275/500, Loss: 0.000982901663519442\n",
      "Epoch: 276/500, Loss: 0.0009249508730135858\n",
      "Epoch: 277/500, Loss: 0.0009753568447194993\n",
      "Epoch: 278/500, Loss: 0.0008495174115523696\n",
      "Epoch: 279/500, Loss: 0.0010082884691655636\n",
      "Epoch: 280/500, Loss: 0.001126001006923616\n",
      "Epoch: 281/500, Loss: 0.0008761720382608473\n",
      "Epoch: 282/500, Loss: 0.0009120340691879392\n",
      "Epoch: 283/500, Loss: 0.0008844936382956803\n",
      "Epoch: 284/500, Loss: 0.0008138881530612707\n",
      "Epoch: 285/500, Loss: 0.0008627871866337955\n",
      "Epoch: 286/500, Loss: 0.0009748531156219542\n",
      "Epoch: 287/500, Loss: 0.0011316637974232435\n",
      "Epoch: 288/500, Loss: 0.000891277042683214\n",
      "Epoch: 289/500, Loss: 0.0008288381504826248\n",
      "Epoch: 290/500, Loss: 0.0010267632314935327\n",
      "Epoch: 291/500, Loss: 0.001061141025274992\n",
      "Epoch: 292/500, Loss: 0.0009288893779739738\n",
      "Epoch: 293/500, Loss: 0.001025949721224606\n",
      "Epoch: 294/500, Loss: 0.0009766838047653437\n",
      "Epoch: 295/500, Loss: 0.000942378188483417\n",
      "Epoch: 296/500, Loss: 0.0011571199866011739\n",
      "Epoch: 297/500, Loss: 0.0009565009968355298\n",
      "Epoch: 298/500, Loss: 0.0007723436574451625\n",
      "Epoch: 299/500, Loss: 0.0009145252406597137\n",
      "Epoch: 300/500, Loss: 0.0009474912076257169\n",
      "Epoch: 301/500, Loss: 0.0008459362434223294\n",
      "Epoch: 302/500, Loss: 0.0007849583053030074\n",
      "Epoch: 303/500, Loss: 0.001075639040209353\n",
      "Epoch: 304/500, Loss: 0.0011005107080563903\n",
      "Epoch: 305/500, Loss: 0.0009303171536885202\n",
      "Epoch: 306/500, Loss: 0.001024645403958857\n",
      "Epoch: 307/500, Loss: 0.0011008208384737372\n",
      "Epoch: 308/500, Loss: 0.0008259557071141899\n",
      "Epoch: 309/500, Loss: 0.000757747096940875\n",
      "Epoch: 310/500, Loss: 0.0009256573393940926\n",
      "Epoch: 311/500, Loss: 0.0010453168069943786\n",
      "Epoch: 312/500, Loss: 0.0009432037477381527\n",
      "Epoch: 313/500, Loss: 0.0008594337268732488\n",
      "Epoch: 314/500, Loss: 0.0010442995699122548\n",
      "Epoch: 315/500, Loss: 0.0009740956593304873\n",
      "Epoch: 316/500, Loss: 0.0008952741627581418\n",
      "Epoch: 317/500, Loss: 0.0009428604389540851\n",
      "Epoch: 318/500, Loss: 0.0009820206323638558\n",
      "Epoch: 319/500, Loss: 0.0009908733190968633\n",
      "Epoch: 320/500, Loss: 0.0010119015350937843\n",
      "Epoch: 321/500, Loss: 0.0009195508318953216\n",
      "Epoch: 322/500, Loss: 0.0008176651317626238\n",
      "Epoch: 323/500, Loss: 0.000951663008891046\n",
      "Epoch: 324/500, Loss: 0.0009106274810619652\n",
      "Epoch: 325/500, Loss: 0.0009248870192095637\n",
      "Epoch: 326/500, Loss: 0.0008505639852955937\n",
      "Epoch: 327/500, Loss: 0.0010337982093915343\n",
      "Epoch: 328/500, Loss: 0.0009694383479654789\n",
      "Epoch: 329/500, Loss: 0.0008402632083743811\n",
      "Epoch: 330/500, Loss: 0.0009117689915001392\n",
      "Epoch: 331/500, Loss: 0.0009491958771832287\n",
      "Epoch: 332/500, Loss: 0.0010140164522454143\n",
      "Epoch: 333/500, Loss: 0.0008719356846995652\n",
      "Epoch: 334/500, Loss: 0.001110021723434329\n",
      "Epoch: 335/500, Loss: 0.0007868369575589895\n",
      "Epoch: 336/500, Loss: 0.0009100931347347796\n",
      "Epoch: 337/500, Loss: 0.0008121200371533632\n",
      "Epoch: 338/500, Loss: 0.0008697666926309466\n",
      "Epoch: 339/500, Loss: 0.0007963238167576492\n",
      "Epoch: 340/500, Loss: 0.0007935453322716057\n",
      "Epoch: 341/500, Loss: 0.0008471353212371469\n",
      "Epoch: 342/500, Loss: 0.0007992947357706726\n",
      "Epoch: 343/500, Loss: 0.0007732773083262146\n",
      "Epoch: 344/500, Loss: 0.0007803637417964637\n",
      "Epoch: 345/500, Loss: 0.0009908139472827315\n",
      "Epoch: 346/500, Loss: 0.0008382481755688787\n",
      "Epoch: 347/500, Loss: 0.0008031083852984011\n",
      "Epoch: 348/500, Loss: 0.0009209367563016713\n",
      "Epoch: 349/500, Loss: 0.0009045131737366319\n",
      "Epoch: 350/500, Loss: 0.0009559594909660518\n",
      "Epoch: 351/500, Loss: 0.0007476476021111012\n",
      "Epoch: 352/500, Loss: 0.0008263844647444785\n",
      "Epoch: 353/500, Loss: 0.0008792817243374884\n",
      "Epoch: 354/500, Loss: 0.0010251272469758987\n",
      "Epoch: 355/500, Loss: 0.0008954384829849005\n",
      "Epoch: 356/500, Loss: 0.0007959879585541785\n",
      "Epoch: 357/500, Loss: 0.0008197515271604061\n",
      "Epoch: 358/500, Loss: 0.0008703379426151514\n",
      "Epoch: 359/500, Loss: 0.0008807969861663878\n",
      "Epoch: 360/500, Loss: 0.0007310860673896968\n",
      "Epoch: 361/500, Loss: 0.0011027437867596745\n",
      "Epoch: 362/500, Loss: 0.000747798359952867\n",
      "Epoch: 363/500, Loss: 0.0010119930375367403\n",
      "Epoch: 364/500, Loss: 0.0008640608284622431\n",
      "Epoch: 365/500, Loss: 0.000807116914074868\n",
      "Epoch: 366/500, Loss: 0.0008573017548769712\n",
      "Epoch: 367/500, Loss: 0.0009810422779992223\n",
      "Epoch: 368/500, Loss: 0.0008767602848820388\n",
      "Epoch: 369/500, Loss: 0.0008150425856001675\n",
      "Epoch: 370/500, Loss: 0.0008929937612265348\n",
      "Epoch: 371/500, Loss: 0.0008383868844248354\n",
      "Epoch: 372/500, Loss: 0.0007573062903247774\n",
      "Epoch: 373/500, Loss: 0.0008086484740488231\n",
      "Epoch: 374/500, Loss: 0.00074016023427248\n",
      "Epoch: 375/500, Loss: 0.0008772347937338054\n",
      "Epoch: 376/500, Loss: 0.000797688087914139\n",
      "Epoch: 377/500, Loss: 0.000718043593224138\n",
      "Epoch: 378/500, Loss: 0.0007932340377010405\n",
      "Epoch: 379/500, Loss: 0.0007861786871217191\n",
      "Epoch: 380/500, Loss: 0.0007902829092927277\n",
      "Epoch: 381/500, Loss: 0.0008052574121393263\n",
      "Epoch: 382/500, Loss: 0.0006664431421086192\n",
      "Epoch: 383/500, Loss: 0.0006624339730478823\n",
      "Epoch: 384/500, Loss: 0.000881875166669488\n",
      "Epoch: 385/500, Loss: 0.0008391252486035228\n",
      "Epoch: 386/500, Loss: 0.0007943547097966075\n",
      "Epoch: 387/500, Loss: 0.0007141566020436585\n",
      "Epoch: 388/500, Loss: 0.0008343800436705351\n",
      "Epoch: 389/500, Loss: 0.0009012476075440645\n",
      "Epoch: 390/500, Loss: 0.0007703305454924703\n",
      "Epoch: 391/500, Loss: 0.0007986962446011603\n",
      "Epoch: 392/500, Loss: 0.000656542950309813\n",
      "Epoch: 393/500, Loss: 0.0007053620065562427\n",
      "Epoch: 394/500, Loss: 0.0008177809650078416\n",
      "Epoch: 395/500, Loss: 0.0010602958500385284\n",
      "Epoch: 396/500, Loss: 0.0008574234088882804\n",
      "Epoch: 397/500, Loss: 0.000668263528496027\n",
      "Epoch: 398/500, Loss: 0.0006498563452623785\n",
      "Epoch: 399/500, Loss: 0.0007692744256928563\n",
      "Epoch: 400/500, Loss: 0.0008157230913639069\n",
      "Epoch: 401/500, Loss: 0.0008239152957685292\n",
      "Epoch: 402/500, Loss: 0.0009624481317587197\n",
      "Epoch: 403/500, Loss: 0.0008257064037024975\n",
      "Epoch: 404/500, Loss: 0.0008411798044107854\n",
      "Epoch: 405/500, Loss: 0.0007683340809307992\n",
      "Epoch: 406/500, Loss: 0.000846889684908092\n",
      "Epoch: 407/500, Loss: 0.0007028200780041516\n",
      "Epoch: 408/500, Loss: 0.0008561181020922959\n",
      "Epoch: 409/500, Loss: 0.0006528744706884027\n",
      "Epoch: 410/500, Loss: 0.0006810646736994386\n",
      "Epoch: 411/500, Loss: 0.0007902004290372133\n",
      "Epoch: 412/500, Loss: 0.0007590571185573936\n",
      "Epoch: 413/500, Loss: 0.00061492231907323\n",
      "Epoch: 414/500, Loss: 0.0009146494558081031\n",
      "Epoch: 415/500, Loss: 0.0007852576090954244\n",
      "Epoch: 416/500, Loss: 0.0007691304199397564\n",
      "Epoch: 417/500, Loss: 0.0008646053029224277\n",
      "Epoch: 418/500, Loss: 0.0006370324990712106\n",
      "Epoch: 419/500, Loss: 0.0007735901162959635\n",
      "Epoch: 420/500, Loss: 0.0005914572393521667\n",
      "Epoch: 421/500, Loss: 0.0007136452477425337\n",
      "Epoch: 422/500, Loss: 0.0009246981935575604\n",
      "Epoch: 423/500, Loss: 0.000767667661421001\n",
      "Epoch: 424/500, Loss: 0.0009580549667589366\n",
      "Epoch: 425/500, Loss: 0.0006974724237807095\n",
      "Epoch: 426/500, Loss: 0.0008091393974609673\n",
      "Epoch: 427/500, Loss: 0.0007568808505311608\n",
      "Epoch: 428/500, Loss: 0.000843220273964107\n",
      "Epoch: 429/500, Loss: 0.0009295624913647771\n",
      "Epoch: 430/500, Loss: 0.0007036070455797017\n",
      "Epoch: 431/500, Loss: 0.0009189628763124347\n",
      "Epoch: 432/500, Loss: 0.0006907964707352221\n",
      "Epoch: 433/500, Loss: 0.0007500692154280841\n",
      "Epoch: 434/500, Loss: 0.0008107870817184448\n",
      "Epoch: 435/500, Loss: 0.0006682376260869205\n",
      "Epoch: 436/500, Loss: 0.0006706361309625208\n",
      "Epoch: 437/500, Loss: 0.0006832045037299395\n",
      "Epoch: 438/500, Loss: 0.0008200692245736718\n",
      "Epoch: 439/500, Loss: 0.0008168489439412951\n",
      "Epoch: 440/500, Loss: 0.0008457984658889472\n",
      "Epoch: 441/500, Loss: 0.0007366530480794609\n",
      "Epoch: 442/500, Loss: 0.0006456050905399024\n",
      "Epoch: 443/500, Loss: 0.000793346029240638\n",
      "Epoch: 444/500, Loss: 0.0006537382141686976\n",
      "Epoch: 445/500, Loss: 0.0007059940253384411\n",
      "Epoch: 446/500, Loss: 0.0007251008646562696\n",
      "Epoch: 447/500, Loss: 0.0007858696044422686\n",
      "Epoch: 448/500, Loss: 0.0006641051732003689\n",
      "Epoch: 449/500, Loss: 0.000646522210445255\n",
      "Epoch: 450/500, Loss: 0.0006862091249786317\n",
      "Epoch: 451/500, Loss: 0.0007120271911844611\n",
      "Epoch: 452/500, Loss: 0.0005835294723510742\n",
      "Epoch: 453/500, Loss: 0.0007519738283008337\n",
      "Epoch: 454/500, Loss: 0.000750318227801472\n",
      "Epoch: 455/500, Loss: 0.0006654910976067185\n",
      "Epoch: 456/500, Loss: 0.0007605291320942342\n",
      "Epoch: 457/500, Loss: 0.0007513879099860787\n",
      "Epoch: 458/500, Loss: 0.0009634186862967908\n",
      "Epoch: 459/500, Loss: 0.0006157723255455494\n",
      "Epoch: 460/500, Loss: 0.0006944133783690631\n",
      "Epoch: 461/500, Loss: 0.0006554206483997405\n",
      "Epoch: 462/500, Loss: 0.0005809941794723272\n",
      "Epoch: 463/500, Loss: 0.0005908810999244452\n",
      "Epoch: 464/500, Loss: 0.0007329214713536203\n",
      "Epoch: 465/500, Loss: 0.0008200487936846912\n",
      "Epoch: 466/500, Loss: 0.0006468810606747866\n",
      "Epoch: 467/500, Loss: 0.0007665037410333753\n",
      "Epoch: 468/500, Loss: 0.0007352178217843175\n",
      "Epoch: 469/500, Loss: 0.0008050377946346998\n",
      "Epoch: 470/500, Loss: 0.0007142705726437271\n",
      "Epoch: 471/500, Loss: 0.0008413306204602122\n",
      "Epoch: 472/500, Loss: 0.0007779681473039091\n",
      "Epoch: 473/500, Loss: 0.00069912284379825\n",
      "Epoch: 474/500, Loss: 0.0007586397114209831\n",
      "Epoch: 475/500, Loss: 0.0005888825980946422\n",
      "Epoch: 476/500, Loss: 0.0007345789344981313\n",
      "Epoch: 477/500, Loss: 0.0007172835757955909\n",
      "Epoch: 478/500, Loss: 0.000607235764618963\n",
      "Epoch: 479/500, Loss: 0.0007950416184030473\n",
      "Epoch: 480/500, Loss: 0.0006477336282841861\n",
      "Epoch: 481/500, Loss: 0.0007659794064238667\n",
      "Epoch: 482/500, Loss: 0.0007628931198269129\n",
      "Epoch: 483/500, Loss: 0.000744581047911197\n",
      "Epoch: 484/500, Loss: 0.0007736700354143977\n",
      "Epoch: 485/500, Loss: 0.0007215215009637177\n",
      "Epoch: 486/500, Loss: 0.0007100744405761361\n",
      "Epoch: 487/500, Loss: 0.0007258213008753955\n",
      "Epoch: 488/500, Loss: 0.0006769465398974717\n",
      "Epoch: 489/500, Loss: 0.000698541640304029\n",
      "Epoch: 490/500, Loss: 0.0006234879256226122\n",
      "Epoch: 491/500, Loss: 0.0006520546739920974\n",
      "Epoch: 492/500, Loss: 0.0007567931315861642\n",
      "Epoch: 493/500, Loss: 0.0007248569745570421\n",
      "Epoch: 494/500, Loss: 0.0006996919983066618\n",
      "Epoch: 495/500, Loss: 0.000690518761985004\n",
      "Epoch: 496/500, Loss: 0.0006306747673079371\n",
      "Epoch: 497/500, Loss: 0.0006516985595226288\n"
     ]
    }
   ],
   "source": [
    "norm_values = get_normalization_values(data)\n",
    "processed_data = preprocess_data(data, norm_values=norm_values)\n",
    "\n",
    "# Set the model parameters\n",
    "input_size = processed_data.shape[2]\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "output_size = processed_data.shape[2]\n",
    "\n",
    "# Initialize the model and loss function\n",
    "model = BiGRU(input_size, hidden_size, num_layers, output_size, dropout=dropout)\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Set the learning rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.cuda()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    processed_data = preprocess_data(data, norm_values=norm_values, batch_size=batch_size)\n",
    "\n",
    "    # Instantiate the custom dataset\n",
    "    dataset = CSVDataset(processed_data)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for batch_idx, (inputs, targets, confidence) in enumerate(dataloader):\n",
    "\n",
    "        # Move tensors to the appropriate device (if using GPU)\n",
    "        inputs, targets, confidence = inputs.to(device), targets.to(device), confidence.to(device)\n",
    "\n",
    "        # Reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Model output\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, targets, confidence)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print the loss for the current batch\n",
    "        print(f\"Epoch: {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-12T20:25:31.522289897Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data = preprocess_for_test(data, norm_values=norm_values)\n",
    "# Instantiate the custom dataset\n",
    "dataset = CSVDataset(processed_data)\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "for batch_idx, (inputs, targets, confidence) in enumerate(dataloader):\n",
    "    inputs, targets, confidence = inputs.to(device), targets.to(device), confidence.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs).cpu()\n",
    "    plot_heatmaps(inputs[0,:,0:8].cpu(), output[0,:,0:8].detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_aug = pd.DataFrame.copy(data)\n",
    "# un-normalize\n",
    "data_aug.loc[1:,:] = normalize_data(output.detach().numpy(), norm_values, forward=False)\n",
    "\n",
    "# if you don't want to interpolate time points with no centerdetect\n",
    "if False:\n",
    "    nan_indices = data.isna().stack()[lambda x: x].index.tolist()\n",
    "    for row, col in nan_indices:\n",
    "        data_aug.at[row, col] = pd.NA\n",
    "\n",
    "data_aug.to_csv(save_folder + '/data3D_smooth.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_heatmaps(model.gru.all_weights[0][0].cpu().detach().numpy(), model.gru.all_weights[0][1].cpu().detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
